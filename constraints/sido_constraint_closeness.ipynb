{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import re\n",
    "import scipy.sparse\n",
    "import scipy.sparse.csgraph\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input_dir（input directory）\n",
    "current_note_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "INPUT_DIR = os.path.join(current_note_path, 'data')\n",
    "\n",
    "# if INPUT_DIR has not been created yet, create it\n",
    "if not os.path.isdir(INPUT_DIR):\n",
    "    os.mkdir(INPUT_DIR)\n",
    "\n",
    "# output_dir(output directory) creation\n",
    "OUTPUT_DIR = os.path.join(current_note_path, 'outputs')\n",
    "\n",
    "# if OUTPUT_DIR has not been created yet, create it\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_path = '/scratch/bell/sido/constraints'\n",
    "# file_location = os.path.join(source_path, '*.net')\n",
    "# filenames = sorted(glob.glob(file_location))\n",
    "# print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you run this code, comment it out\n",
    "# move csv files to `data` directory(=folder)\n",
    "unique_dir_names = []\n",
    "for f in Path(f'{current_note_path}').rglob('*.net'):\n",
    "    unique_dir_names.append(f)\n",
    "for g in Path(f'{file_location}').rglob('*.net'):\n",
    "    unique_dir_names.append(g)\n",
    "\n",
    "for file in list(set(unique_dir_names)):\n",
    "    print(f'moved file: {file}')\n",
    "    shutil.move(f'{file}', f'{INPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/Users/yuliyu/Dropbox/Network_VC_2023/apr2023/quarterlyversion/Pajek')\n",
    "# os.chdir('programming/RA/vc_syn_5yr1980q1.net')\n",
    "# os.chdir('/scratch/bell/hu244/vxpert/quarterlyversion')\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  print the whole path name\n",
    "first_file = filenames[0]\n",
    "print(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the file name\n",
    "file_name = os.path.basename(first_file)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiprocessing for-loop to return a closeness centrality for each year as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the directory containing the files\n",
    "path = INPUT_DIR\n",
    "\n",
    "# get a list of all the files in the directory\n",
    "files = os.listdir(path)\n",
    "\n",
    "# filter the list to include only the .net files\n",
    "files = [f for f in files if f.endswith(\".net\")]\n",
    "\n",
    "# define a function to process a single file\n",
    "def process_file(filename):\n",
    "    # extract the name of the file without the extension\n",
    "    name = re.search(\"5yr(.+?).net\", filename).group(1)\n",
    "    \n",
    "    # read the graph from the file\n",
    "    G = nx.read_pajek(os.path.join(path, filename))\n",
    "    \n",
    "    # convert the graph to an undirected graph\n",
    "    G = G.to_undirected()\n",
    "    \n",
    "    # calculate the closeness centrality of each node\n",
    "    A = nx.adjacency_matrix(G).tolil()\n",
    "    D = scipy.sparse.csgraph.floyd_warshall(A, directed=False, unweighted=False)\n",
    "    N = D.shape[0] # N: number of all nodes\n",
    "    closeness_centrality = {}\n",
    "    for r in range(0, N):\n",
    "        cc = 0.0\n",
    "        possible_paths = list(enumerate(D[r, :]))\n",
    "        shortest_paths = dict(filter(lambda x: not x[1] == np.inf, possible_paths)) # shortest path from node u\n",
    "        total = sum(shortest_paths.values())\n",
    "        n_shortest_paths = len(shortest_paths) - 1.0  \n",
    "        if total > 0.0 and N > 1:\n",
    "            s = n_shortest_paths / (N - 1)\n",
    "            cc = (n_shortest_paths / total) * s\n",
    "        closeness_centrality[r] = cc\n",
    "    \n",
    "    # create a DataFrame from the closeness centrality dictionary\n",
    "    df = pd.DataFrame.from_dict(closeness_centrality, orient=\"index\", columns=[\"closeness\"])\n",
    "    df.index.name = \"node\"\n",
    "    \n",
    "    # save the DataFrame to a CSV file\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, f\"closeness_{name}.csv\"), index=False)\n",
    "\n",
    "# create a process pool executor with 4 worker processes\n",
    "def main():\n",
    "    # When you want to know the progress in tqdm, you need to specify the total number of files.\n",
    "    with tqdm(total=len(files)) as progress:\n",
    "        with ProcessPoolExecutor(max_workers=os.cpu_count() // 2) as executor:\n",
    "            # submit a task for each file to the executor\n",
    "            futures = [executor.submit(process_file, filename).add_done_callback(lambda p: progress.update()) for filename in files]\n",
    "\n",
    "            # wait for all the tasks to complete\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"Files are processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cathy's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myoutputpath = \"/scratch/bell/hu244/vxpert/quarterlyversion/PajekOutput/\"\n",
    "myoutputpath = OUTPUT_DIR\n",
    "\n",
    "# i = 0\n",
    "# end = end\n",
    "end =174\n",
    "print(end)\n",
    "\n",
    "for i in range(0, end):\n",
    "# while i <end:\n",
    "# for i in range(-1,163):\n",
    "   \n",
    "    if i == end:\n",
    "        break\n",
    "#     print(filenames[i])\n",
    "    #group(0) = \"\"5yr(.+?).net\"\", group(1)= \"(.+?)\"\n",
    "    name = re.search(\"5yr(.+?).net\",filenames[i]).group(1)\n",
    "    G = nx.read_pajek(filenames[i])\n",
    "    print(\"Processing #\", i , \"file\")\n",
    "    print(name)\n",
    "    print(G) #检查 是否读了所有的 .net files\n",
    "    # undirected graph\n",
    "    G1 = nx.Graph(G)\n",
    "    # returns the number of vertices (nodes)\n",
    "    n = nx.number_of_nodes(G1)\n",
    "\n",
    "    # returns adjacency matrix of G.\n",
    "    ## creates a sparse matrix to save up the memory with COO method\n",
    "    A = nx.adjacency_matrix(G, format=\"csr\").tolil()\n",
    "    D = scipy.sparse.csgraph.floyd_warshall(\n",
    "            A, directed=False, unweighted=False)\n",
    "\n",
    "    N = D.shape[0] # N: number of all nodes\n",
    "    closeness_centrality = {}\n",
    "    for r in range(0, N):\n",
    "        cc = 0.0\n",
    "        possible_paths = list(enumerate(D[r, :]))\n",
    "        shortest_paths = dict(filter(lambda x: not x[1] == np.inf, possible_paths)) # shortest path from node u\n",
    "        total = sum(shortest_paths.values())\n",
    "        # n = len(shortest_paths)\n",
    "        n_shortest_paths = len(shortest_paths) - 1.0  \n",
    "        if total > 0.0 and N > 1:\n",
    "            s = n_shortest_paths / (N - 1)\n",
    "            cc = (n_shortest_paths / total) * s\n",
    "        closeness_centrality[r] = cc\n",
    "    closeness_df = pd.DataFrame()\n",
    "    closeness_df[\"nodes\"] = closeness_centrality.keys()\n",
    "    closeness_df[\"closeness\"] = closeness_centrality.values()\n",
    "\n",
    "    newfile_closeness = \"\".join([myoutputpath,\"closeness_\", name,\".csv\"])\n",
    "    closeness_df.to_csv(newfile_closeness, index=False)\n",
    "    i += 1\n",
    "else:\n",
    "    print(\"Files are processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myoutputpath = \"/scratch/bell/hu244/vxpert/quarterlyversion/PajekOutput/\"\n",
    "myoutputpath = OUTPUT_DIR\n",
    "\n",
    "# i = 0\n",
    "# end = end\n",
    "end =174\n",
    "print(end)\n",
    "\n",
    "for i in range(0, end):\n",
    "# while i <end:\n",
    "# for i in range(-1,163):\n",
    "   \n",
    "    if i == end:\n",
    "        break\n",
    "#     print(filenames[i])\n",
    "    #group(0) = \"\"5yr(.+?).net\"\", group(1)= \"(.+?)\"\n",
    "    name = re.search(\"5yr(.+?).net\",filenames[i]).group(1)\n",
    "    G = nx.read_pajek(filenames[i])\n",
    "    print(\"Processing #\", i , \"file\")\n",
    "    print(name)\n",
    "    print(G) #检查 是否读了所有的 .net files\n",
    "    # undirected graph\n",
    "    G1 = nx.Graph(G)\n",
    "    # returns the number of vertices (nodes)\n",
    "    n = nx.number_of_nodes(G1)\n",
    "#     constraint = nx.constraint(G, nodes=None, weight=None)\n",
    "#     cons_df = pd.DataFrame()\n",
    "#     cons_df[\"node\"] = constraint.keys()\n",
    "#     cons_df[\"constraint\"] = constraint.values()\n",
    "#     newfile=\"\".join([myoutputpath,\"constraint_\", name,\".csv\"])\n",
    "#     print(type(newfile))\n",
    "#     cons_df.to_csv(newfile, index=False)\n",
    "    # returns adjacency matrix of G.\n",
    "    ## creates a sparse matrix to save up the memory with COO method\n",
    "    A = nx.adjacency_matrix(G, format=\"csr\").tolil()\n",
    "    D = scipy.sparse.csgraph.floyd_warshall(\n",
    "            A, directed=False, unweighted=False)\n",
    "\n",
    "    N = D.shape[0] # N: number of all nodes\n",
    "    closeness_centrality = {}\n",
    "    for r in range(0, N):\n",
    "        cc = 0.0\n",
    "        possible_paths = list(enumerate(D[r, :]))\n",
    "        shortest_paths = dict(filter(lambda x: not x[1] == np.inf, possible_paths)) # shortest path from node u\n",
    "\n",
    "        \n",
    "        total = sum(shortest_paths.values())\n",
    "        # n = len(shortest_paths)\n",
    "        n_shortest_paths = len(shortest_paths) - 1.0  \n",
    "        if total > 0.0 and N > 1:\n",
    "            s = n_shortest_paths / (N - 1)\n",
    "            cc = (n_shortest_paths / total) * s\n",
    "        closeness_centrality[r] = cc\n",
    "    closeness_df = pd.DataFrame()\n",
    "    closeness_df[\"nodes\"] = closeness_centrality.keys()\n",
    "    closeness_df[\"closeness\"] = closeness_centrality.values()\n",
    "    \n",
    "#     closeness = nx.closeness_centrality(G, u=None, distance='weight', wf_improved=True)\n",
    "#     closeness_df = pd.DataFrame()\n",
    "#     closeness_df[\"nodes\"] = closeness.keys()\n",
    "#     closeness_df[\"closeness\"] = closeness.values()\n",
    "    newfile_closeness = \"\".join([myoutputpath,\"closeness_\", name,\".csv\"])\n",
    "    closeness_df.to_csv(newfile_closeness, index=False)\n",
    "    i += 1\n",
    "else:\n",
    "    print(\"Files are processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10:01PM 12 665 nodes\n",
    "# 10:14pm 13 707 nodes\n",
    "# 10: 46pm 15 808 nodes\n",
    "# 12:46 18 988 nodes\n",
    "# 1:39 19 1038 nodes\n",
    "\n",
    "# requested time for jupter ran out. \n",
    "\n",
    "# 2:23am 19 1038 nodes\n",
    "# 在电脑熄屏之后不能继续\n",
    "# 5/4 12:48pm 25 1248 nodes\n",
    "\n",
    "#5/4 6:38pm #91 2002q4\n",
    "#    6:49pm 99  2004q4  5477 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_pajek(filenames[0])\n",
    "print(G) #检查 是否读了所有的 .net files\n",
    "G1 = nx.Graph(G)\n",
    "n = nx.number_of_nodes(G1)\n",
    "constraint = nx.constraint(G, nodes=None, weight=None)\n",
    "print(type(constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Network Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = 1980\n",
    "year_end = 1980\n",
    "\n",
    "missing_list = []\n",
    "\n",
    "myoutputpath = \"/scratch/bell/hu244/vxpert/quarterlyversion/Stataoutput/PajekOutput\"\n",
    "\n",
    "year = year_start\n",
    "while year <= year_end:\n",
    "    for i in range(0,4):\n",
    "        i += 1\n",
    "\n",
    "        filename = \"vc_syn_5yr\" + str(year) + \"q\" + str(i) + \".net\"\n",
    "#         print(filename)\n",
    "        name = str(year) +\"q\" +str(i)\n",
    "        print(name)\n",
    "        #Check if file is exist\n",
    "        if os.path.isfile(filename) == True:\n",
    "            print(\"File exist, processing\")\n",
    "        else:\n",
    "            print(filename,\"doesn't exist, skipped\")\n",
    "            missing_list.append(filename)\n",
    "            continue\n",
    "        #Create files if exist\n",
    "        G = nx.read_pajek(filename)\n",
    "        G1 = nx.Graph(G)\n",
    "        n = nx.number_of_nodes(G1)\n",
    "#         constraint = nx.constraint(G, nodes=None, weight=None)\n",
    "#         cons_df = pd.DataFrame()\n",
    "#         cons_df[\"node\"] = constraint.keys()\n",
    "#         cons_df[\"constraint\"] = constraint.values()\n",
    "#         newfile=\"\".join([myoutputpath,\"constraint_\", name,\".csv\"])\n",
    "#         cons_df.to_csv(newfile, index=False)\n",
    "#         closeness = nx.closeness_centrality(G, u=None, distance='weight', wf_improved=True)\n",
    "#         closeness_df = pd.DataFrame()\n",
    "#         closeness_df[\"nodes\"] = closeness.keys()\n",
    "#         closeness_df[\"closeness\"] = closeness.values()\n",
    "#         newfile_closeness = \"\".join([myoutputpath,\"closeness_\", name,\".csv\"])\n",
    "#         closeness_df.to_csv(newfile_closeness, index=False)\n",
    "        effective_size = nx.effective_size(G, nodes=None, weight='weight')\n",
    "        effectivesize_df = pd.DataFrame()\n",
    "        effectivesize_df[\"node\"]=effective_size.keys()\n",
    "        effectivesize_df[\"effective_size\"]=effective_size.values()\n",
    "        newfile_effsize=\"\"join([myoutputpath,\"effectivesize_\",name ,\".csv\"])\n",
    "        effectivsize_df.to_csv(newfile_effsize, index=False)\n",
    "        avg_neighdegree = nx.average_neighbor_degree(G, nodes=None, weight='weight')\n",
    "        avgneidegree_df = pd.DataFrame()\n",
    "        avgneidegree_df[\"node\"]=avg_neighdegree.keys()\n",
    "        avgneidegree_df[\"average_neighbor_degree\"]=avg_neighdegree.values()\n",
    "        newfile_avgneidegree = \"\".join([myoutputpath, \"avgneidegree\", name, \".csv\"])\n",
    "        avgneidegree_df.to_csv(newfile_avgneidegree, index=False)\n",
    "    year += 1\n",
    "else:\n",
    "    print(\"*********Loop is over*********\")\n",
    "    print(\"The missing files:\")\n",
    "    print(missing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint - Cathy's loop\n",
    "import time\n",
    "\n",
    "\n",
    "year_start = 1986\n",
    "year_end = 1986\n",
    "\n",
    "missing_list = []\n",
    "\n",
    "myoutputpath = \"/Users/kuanchensu/Documents/RA/network/\"\n",
    "\n",
    "year = year_start\n",
    "while year <= year_end:\n",
    "    for i in range(0,4):\n",
    "        i += 1\n",
    "        start_time1 = time.time()\n",
    "        print(start_time1)\n",
    "        filename = \"vc_syn_5yr\" + str(year) + \"q\" + str(i) + \".net\"\n",
    "        name = \"vc_syn_5yr\" + str(year) + \"q\" + str(i)\n",
    "        print(filename)\n",
    "        #Check if file is exist\n",
    "        if os.path.isfile(filename) == True:\n",
    "            print(\"File exist, processing\")\n",
    "        else:\n",
    "            print(filename,\"doesn't exist, skipped\")\n",
    "            missing_list.append(filename)\n",
    "            continue\n",
    "        start_time2 = time.time()\n",
    "        print(\"time2\", start_time2)\n",
    "        #Create files if exist\n",
    "        G = nx.read_pajek(filename)\n",
    "        G1 = nx.Graph(G)\n",
    "        n = nx.number_of_nodes(G1)\n",
    "        start_time3 = time.time()\n",
    "        print(\"time3\", start_time3)\n",
    "#         constraint = nx.constraint(G, nodes=None, weight=None)\n",
    "#         cons_df = pd.DataFrame()\n",
    "#         cons_df[\"node\"] = constraint.keys()\n",
    "#         cons_df[\"constraint\"] = constraint.values()\n",
    "#         newfile=\"\".join([myoutputpath,\"constraint_\", filename,\".csv\"])\n",
    "#         start_time4 = time.time()\n",
    "#         print(\"time4\", start_time4)\n",
    "#         cons_df.to_csv(newfile, index=False)\n",
    "#         start_time5 = time.time()\n",
    "#         print(\"time5\", start_time5)\n",
    "#         closeness = nx.closeness_centrality(G, u=None, distance='weight', wf_improved=True)\n",
    "        A = nx.adjacency_matrix(G).tolil()\n",
    "        D = scipy.sparse.csgraph.floyd_warshall( \\\n",
    "                     A, directed=False, unweighted=False)\n",
    "\n",
    "        n = D.shape[0]\n",
    "        closeness_centrality = {}\n",
    "        for r in range(0, n):\n",
    "\n",
    "            cc = 0.0\n",
    "\n",
    "            possible_paths = list(enumerate(D[r, :]))\n",
    "            shortest_paths = dict(filter( \\\n",
    "                lambda x: not x[1] == np.inf, possible_paths))\n",
    "\n",
    "            total = sum(shortest_paths.values())\n",
    "            n_shortest_paths = len(shortest_paths) - 1.0\n",
    "            if total > 0.0 and n > 1:\n",
    "                s = n_shortest_paths / (n - 1)\n",
    "                cc = (n_shortest_paths / total) * s\n",
    "            closeness_centrality[r] = cc\n",
    "        closeness_df = pd.DataFrame()\n",
    "        closeness_df[\"nodes\"] = closeness_centrality.keys()\n",
    "        closeness_df[\"closeness\"] = closeness_centrality.values()\n",
    "        newfile_closeness = \"\".join([myoutputpath,\"closeness_\", name,\".csv\"])\n",
    "        start_time6 = time.time()\n",
    "        print(\"time6\", start_time6)\n",
    "        closeness_df.to_csv(newfile_closeness, index=False)\n",
    "        start_time7 = time.time()\n",
    "        print(\"time7\", start_time7)\n",
    "    year += 1\n",
    "else:\n",
    "    print(\"*********Loop is over*********\")\n",
    "    print(\"The missing files:\")\n",
    "    print(missing_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert txt (from pajek) to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('/scratch/bell/hu244/vxpert/quarterlyversion/StataOutput/PajekOutput')\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert txt to csv\n",
    "# Loop by time:\n",
    "\n",
    "year_start = 1991\n",
    "year_end = 1991\n",
    "\n",
    "missing_list = []\n",
    "\n",
    "myoutputpath = \"/scratch/bell/hu244/vxpert/quarterlyversion/StataOutput/PajekOutput/\"\n",
    "\n",
    "year = year_start\n",
    "while year <= year_end:\n",
    "    for i in range(0,4):\n",
    "        i += 1\n",
    "        start_time1 = time.time()\n",
    "        print(start_time1)\n",
    "        filename = \"constraint_\" + str(year) + \"q\" + str(i) + \".txt\"\n",
    "        name = \"constraint_\" + str(year) + \"q\" + str(i) + \".csv\"\n",
    "        print(filename)\n",
    "        #Check if file is exist\n",
    "        if os.path.isfile(filename) == True:\n",
    "            print(\"File exist, processing\")\n",
    "        else:\n",
    "            print(filename,\"doesn't exist, skipped\")\n",
    "            missing_list.append(filename)\n",
    "            continue\n",
    "        data = pd.read_csv(filename,\n",
    "                   skiprows=1,\n",
    "                   sep='\\t',\n",
    "                   header=None,\n",
    "                   engine='python')\n",
    "        new = data.iloc[:,[1,2]]\n",
    "        new = new.rename(columns={1: \"nodes\", 2: \"constraints\"})\n",
    "        new.to_csv(name, index=False)\n",
    "\n",
    "    year += 1\n",
    "else:\n",
    "    print(\"*********Loop is over*********\")\n",
    "    print(\"The missing files:\")\n",
    "    print(missing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loop by file name:\n",
    "import re\n",
    "\n",
    "files = [\"constraint_1988q3.txt\", \"constraint_1989q1.txt\"]\n",
    "\n",
    "for filename in files:\n",
    "    data = pd.read_csv(filename,\n",
    "                       skiprows=1,\n",
    "                       sep='\\t',\n",
    "                       header=None,\n",
    "                       engine='python')\n",
    "    new = data.iloc[:,[1,2]]\n",
    "    new = new.rename(columns={1: \"nodes\", 2: \"constraints\"})\n",
    "    name = re.search(\"(.+?).txt\",filename).group(1)\n",
    "    name = \"\".join([name, \".csv\"])\n",
    "    print(name)\n",
    "    new.to_csv(name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
